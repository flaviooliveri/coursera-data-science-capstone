---
title: "Data Science Specialization SwiftKey Capstone"
subtitle: "Week 2 Milestone Report"
author: "Flavio Oliveri"
date: "2021/08/02"
output:
  html_document:
    keep_md: false
---

## Introduction

The objective is to explore the datasets that are going to be used to build the prediction model. Basic exploratory analysis of the files and some data preparation like cleaning and sampling are going to be performed.

The idea is to build a corpus based on three files with text extracted from blogs, news and twitter.

## Load the data

Download and load the data.

```{r , echo = TRUE}
zipFile <- "files/Coursera-SwiftKey.zip"
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

if (!file.exists('files')) {
    dir.create('files')
}
if (!file.exists(zipFile)) {
    tempFile <- tempfile()
    download.file(url, zipFile)
    unlink(tempFile)
}

if (!file.exists("files/final/en_US")) {
    unzip(zipFile, exdir = "files")
}

blogsFile <- "files/final/en_US/en_US.blogs.txt"
conn <- file(blogsFile, open = "r")
blogs <- readLines(conn, encoding = "UTF-8", skipNul = TRUE)
close(conn)

newsFile <- "files/final/en_US/en_US.news.txt"
conn <- file(newsFile, open = "r")
news <- readLines(conn, encoding = "UTF-8", skipNul = TRUE)
close(conn)

twitterFile <- "files/final/en_US/en_US.twitter.txt"
conn <- file(twitterFile, open = "r")
twitter <- readLines(conn, encoding = "UTF-8", skipNul = TRUE)
close(conn)

rm(conn)
```
### Files Summary

```{r, echo = FALSE, results = 'hold'}
library(stringi)
library(kableExtra)

fileSize <- round(file.info(c(blogsFile, newsFile, twitterFile))$size / 1024 ^ 2)
numLines <- sapply(list(blogs, news, twitter), length)
numWords <- sapply(list(blogs, news, twitter), stri_stats_latex)[4,]
numChars <- sapply(list(nchar(blogs), nchar(news), nchar(twitter)), sum)

wplSummary <- sapply(list(blogs, news, twitter),
             function(x) summary(stri_count_words(x))[c('Mean', 'Max.')])
rownames(wplSummary) <- c('WordPerLine Mean', 'WordPerLine Max')

summary <- data.frame(
    File = c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt"),
    FileSize = paste(fileSize, " MB"),
    Lines = numLines,
    Words = numWords,
    Characters = numChars,
    t(rbind(round(wplSummary)))
)

kable(summary, align = c("l", rep("r", 7)), caption = "") %>% kable_styling(position = "left")
```
The first obvious conclusion is that, as expected, Twitter has fewer words per line than News and News less than Blogs.

Also, the size of the files are big so sampling is going to be needed.

## Sampling and cleaning

The three data sets will be sampled at 5% and non ASCII characters will be removed. A new file with the 3 samples will be created.

```{r , echo = TRUE, message = FALSE}
sampleDataFile <- "files/final/en_US/sample.txt"

percentage <- 0.05
set.seed(450134)
sampleBlogs <- sample(blogs, length(blogs) * percentage, replace = FALSE)
sampleNews <- sample(news, length(news) * percentage, replace = FALSE)
sampleTwitter <- sample(twitter, length(twitter) * percentage, replace = FALSE)
sampleBlogs <- iconv(sampleBlogs, "latin1", "ASCII", sub = "")
sampleNews <- iconv(sampleNews, "latin1", "ASCII", sub = "")
sampleTwitter <- iconv(sampleTwitter, "latin1", "ASCII", sub = "")
allSamples <- c(sampleBlogs, sampleNews, sampleTwitter)
conn <- file(sampleDataFile, open = "w")
writeLines(allSamples, conn)
close(conn)


```


## Corpus

To build the corpus I'm going to apply these transformations:

 - Remove URL and email patterns
 - Remove stop words
 - Remove numbers
 - Convert to lowercase
 - Remove punctuation

The result is going to be saved into disk.

```{r, echo = FALSE, message = FALSE}
library(tm)
corpusFile <- "files/final/en_US/corpus.txt"

applyTransformations <- function (aDataset) {
    docs <- VCorpus(VectorSource(aDataset))
    toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))

    docs <- tm_map(docs, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
    docs <- tm_map(docs, toSpace, "@[^\\s]+")
    docs <- tm_map(docs, toSpace, "\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b")

    docs <- tm_map(docs, tolower)
    docs <- tm_map(docs, removeWords, stopwords("english"))
    docs <- tm_map(docs, removePunctuation)
    docs <- tm_map(docs, removeNumbers)
    docs <- tm_map(docs, stripWhitespace)
    docs <- tm_map(docs, PlainTextDocument)
    return(docs)
}

corpus <- applyTransformations(allSamples)
corpusText <- data.frame(text = unlist(sapply(corpus, '[', "content")), stringsAsFactors = FALSE)
conn <- file(corpusFile, open = "w")
writeLines(corpusText$text, conn)
close(conn)
```
### N-Gram

Uniqrams, Bigrams, and Trigrams are going to be used in the prediction model.
The following plots are an exploration of these.
```{r message = FALSE, echo = FALSE}
library(RWeka)
library(ggplot2)

unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
```

```{r, message = FALSE, echo = FALSE}
unigramMatrix <- TermDocumentMatrix(corpus, control = list(tokenize = unigramTokenizer))
unigramMatrixFreq <- sort(rowSums(as.matrix(removeSparseTerms(unigramMatrix, 0.99))), decreasing = TRUE)
unigramMatrixFreq <- data.frame(word = names(unigramMatrixFreq), freq = unigramMatrixFreq)
g <- ggplot(unigramMatrixFreq[1:20,], aes(x = reorder(word, -freq), y = freq))
g <- g + geom_bar(stat = "identity", fill = I("grey50"))
g <- g + geom_text(aes(label = freq ), vjust = -0.20, size = 3)
g <- g + xlab("")
g <- g + ylab("Frequency")
g <- g + theme(plot.title = element_text(size = 14, hjust = 0.5, vjust = 0.5),
               axis.text.x = element_text(hjust = 1.0, angle = 45),
               axis.text.y = element_text(hjust = 0.5, vjust = 0.5))
g <- g + ggtitle("Unigrams")
print(g)
```

```{r exploratory-data-analysis-tokenize-bigrams, message = FALSE, echo = FALSE}
bigramMatrix <- TermDocumentMatrix(corpus, control = list(tokenize = bigramTokenizer))
bigramMatrixFreq <- sort(rowSums(as.matrix(removeSparseTerms(bigramMatrix, 0.999))), decreasing = TRUE)
bigramMatrixFreq <- data.frame(word = names(bigramMatrixFreq), freq = bigramMatrixFreq)
g <- ggplot(bigramMatrixFreq[1:20,], aes(x = reorder(word, -freq), y = freq))
g <- g + geom_bar(stat = "identity", fill = I("grey50"))
g <- g + geom_text(aes(label = freq ), vjust = -0.20, size = 3)
g <- g + xlab("")
g <- g + ylab("Frequency")
g <- g + theme(plot.title = element_text(size = 14, hjust = 0.5, vjust = 0.5),
               axis.text.x = element_text(hjust = 1.0, angle = 45),
               axis.text.y = element_text(hjust = 0.5, vjust = 0.5))
g <- g + ggtitle("Bigrams")
print(g)
```

```{r, message = FALSE, echo = FALSE}
trigramMatrix <- TermDocumentMatrix(corpus, control = list(tokenize = trigramTokenizer))
trigramMatrixFreq <- sort(rowSums(as.matrix(removeSparseTerms(trigramMatrix, 0.9999))), decreasing = TRUE)
trigramMatrixFreq <- data.frame(word = names(trigramMatrixFreq), freq = trigramMatrixFreq)
g <- ggplot(trigramMatrixFreq[1:20,], aes(x = reorder(word, -freq), y = freq))
g <- g + geom_bar(stat = "identity", fill = I("grey50"))
g <- g + geom_text(aes(label = freq ), vjust = -0.20, size = 3)
g <- g + xlab("")
g <- g + ylab("Frequency")
g <- g + theme(plot.title = element_text(size = 14, hjust = 0.5, vjust = 0.5),
               axis.text.x = element_text(hjust = 1.0, angle = 45),
               axis.text.y = element_text(hjust = 0.5, vjust = 0.5))
g <- g + ggtitle("Trigrams")
print(g)
```
